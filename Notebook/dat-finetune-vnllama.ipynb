{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30665,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Download pretrain models\n# !conda install -y gdown\n# !gdown 1waTUIQb6BSmgfXG__a8oz6w1K_P7VbpT","metadata":{"id":"8nfEWbgGOLiQ","execution":{"iopub.status.busy":"2024-03-04T23:12:40.200034Z","iopub.execute_input":"2024-03-04T23:12:40.201036Z","iopub.status.idle":"2024-03-04T23:12:40.206199Z","shell.execute_reply.started":"2024-03-04T23:12:40.200994Z","shell.execute_reply":"2024-03-04T23:12:40.205114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!git clone https://github.com/liamnguyen97/finetune_LLM.git\n%cd finetune_LLM\n!pip install -r requirements.txt -q\n!pip install datasets==2.14.6 -q","metadata":{"id":"2nt-AnkGObza","execution":{"iopub.status.busy":"2024-03-04T23:12:40.208121Z","iopub.execute_input":"2024-03-04T23:12:40.208540Z","iopub.status.idle":"2024-03-04T23:13:19.659415Z","shell.execute_reply.started":"2024-03-04T23:12:40.208509Z","shell.execute_reply":"2024-03-04T23:13:19.658255Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from huggingface_hub import login\n\naccess_token = \"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\"\n\nlogin(access_token)","metadata":{"id":"bWTr7zsVPR-c","execution":{"iopub.status.busy":"2024-03-04T23:13:19.661687Z","iopub.execute_input":"2024-03-04T23:13:19.662022Z","iopub.status.idle":"2024-03-04T23:13:20.693207Z","shell.execute_reply.started":"2024-03-04T23:13:19.661992Z","shell.execute_reply":"2024-03-04T23:13:20.692190Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile dataset.py\nfrom datasets import load_dataset\nfrom torch.utils.data import DataLoader\nfrom transformers import DataCollatorForSeq2Seq\nimport re\n\n\nclass Dataset:\n    def __init__(\n        self,\n        tokenizer,\n        dataset_name: str,\n        batch_size: int,\n        ):\n        self.tokenizer = tokenizer\n        self.dataset_name = dataset_name\n        self.batch_size = batch_size\n\n    def load_data(self, dataset_name):\n        dataset = load_dataset(\n            self.dataset_name,\n            \"main_data\",\n            split=\"train\",\n            use_auth_token=True,\n        )\n        return dataset\n\n    def tokenize(self, text, max_length=4096, add_eos_token=True):\n        result = self.tokenizer(\n            text,\n            truncation=True,\n            max_length=max_length,\n            padding=False,\n            return_tensors=None\n        )\n        if (\n            result[\"input_ids\"][-1] != self.tokenizer.eos_token_id\n            and len(result[\"input_ids\"]) < max_length\n            and add_eos_token\n            ):\n\n            result[\"input_ids\"].append(self.tokenizer.eos_token_id)\n            result[\"attention_mask\"].append(1)\n\n        result[\"labels\"] = result[\"input_ids\"].copy()\n        return result\n\n    def generate_and_tokenize_prompt(self, dataset):\n        if dataset['CONTEXT'] is None: dataset['CONTEXT'] = \"\"\n        if dataset['QUESTION'] is None: dataset['QUESTION'] = \"\"\n        if dataset['ANSWER'] is None: dataset['ANSWER'] = \"\"\n        _prompt = \"### Human:\" + dataset['CONTEXT'] + \". Question:\" + dataset[\"QUESTION\"] + \". ### Assistant:\" + dataset[\"ANSWER\"]\n        return self.tokenize(_prompt)\n\n    def dataloader(self):\n        dataset = self.load_data(self.dataset_name)\n        dataset = dataset.shuffle()\n        dataset = dataset.train_test_split(test_size=0.05, seed=42)\n\n        train_data = dataset[\"train\"].map(self.generate_and_tokenize_prompt, num_proc=13)\n        valid_data = dataset[\"test\"].map(self.generate_and_tokenize_prompt, num_proc=13)\n\n        train_data = train_data.remove_columns([\"CONTEXT\", \"QUESTION\", \"ANSWER\"])\n        valid_data = valid_data.remove_columns([\"CONTEXT\", \"QUESTION\", \"ANSWER\"])\n\n        train_data.set_format(\"torch\")\n        valid_data.set_format(\"torch\")\n\n        train_dataloader = DataLoader(\n            train_data,\n            batch_size = self.batch_size,\n            collate_fn = DataCollatorForSeq2Seq(\n                tokenizer = self.tokenizer,\n                padding = True,\n                return_tensors = \"pt\",\n                ),\n            )\n\n        valid_dataloader = DataLoader(\n            valid_data,\n            batch_size = self.batch_size,\n            collate_fn = DataCollatorForSeq2Seq(\n                tokenizer = self.tokenizer,\n                padding = True,\n                return_tensors = \"pt\",\n                ),\n            )\n        return train_dataloader, valid_dataloader\n","metadata":{"id":"cOnIq8SlPVyT","outputId":"14b86521-2532-457b-cd31-83aebcbba8e6","execution":{"iopub.status.busy":"2024-03-04T23:20:10.484442Z","iopub.execute_input":"2024-03-04T23:20:10.484893Z","iopub.status.idle":"2024-03-04T23:20:10.493826Z","shell.execute_reply.started":"2024-03-04T23:20:10.484856Z","shell.execute_reply":"2024-03-04T23:20:10.492776Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile train.py\nimport math\n\nimport torch\nfrom transformers import (\n    AutoConfig,\n    AutoTokenizer,\n    AutoModelForCausalLM,\n    BitsAndBytesConfig,\n    get_scheduler,\n)\nfrom peft import (\n    LoraConfig,\n    get_peft_model,\n)\nfrom tqdm.auto import tqdm\n\nfrom dataset import Dataset\n\n\nclass Trainer:\n    def __init__(\n        self,\n        model_name: str,\n        dataset_name: str,\n        num_epochs: int,\n        batch_size: int = 1,\n        logging_step: int = 50,\n        use_peft: bool = False,\n        use_4bit: bool = False,\n        ):\n        self.model_name = model_name\n        self.dataset_name = dataset_name\n        self.num_epochs = num_epochs\n        self.logging_step = logging_step\n        self.batch_size = batch_size\n        self.use_peft = use_peft\n        self.use_4bit = use_4bit\n        self.device = torch.device(\"cuda\") if torch.cuda.is_available else torch.device(\"cpu\")\n\n    def load_tokenizer(self, model_name):\n        config = AutoConfig.from_pretrained(model_name)\n        architecture = config.architectures[0]\n        tokenizer = AutoTokenizer.from_pretrained(model_name)\n        if \"Llama\" in architecture:\n            tokenizer.add_special_tokens(\n                {\n                    \"eos_token\": \"</s>\",\n                    \"bos_token\": \"</s>\",\n                    \"unk_token\": \"</s>\",\n                }\n            )\n            tokenizer.pad_token_id = 0\n        return tokenizer\n\n\n    def load_model(self, model_name):\n        if self.use_4bit == True:\n            bnb_config = BitsAndBytesConfig(\n                load_in_4bit=True,\n                bnb_4bit_use_double_quant=True,\n                bnb_4bit_quant_type=\"nf4\",\n                bnb_4bit_compute_dtype=torch.float16,\n                )\n            model = AutoModelForCausalLM.from_pretrained(\n                model_name,\n                quantization_config=bnb_config,\n                device_map=\"auto\",\n            )\n        else:\n            model = AutoModelForCausalLM.from_pretrained(model_name)\n            model.to(self.device)\n\n        if self.use_peft == True:\n            lora_config = LoraConfig(\n                r = 8,\n                lora_alpha = 32,\n                lora_dropout = 0.05,\n                bias = \"none\",\n                task_type = \"CAUSAL_LM\",\n                )\n            model = get_peft_model(model, lora_config)\n\n        return model\n\n    def train(self):\n        tokenizer = self.load_tokenizer(self.model_name)\n        model = self.load_model(self.model_name)\n\n        train_dataloader, valid_dataloader = Dataset(\n            tokenizer = tokenizer,\n            dataset_name = self.dataset_name,\n            batch_size = self.batch_size,\n        ).dataloader()\n\n\n        if self.use_peft == True:\n            lr = 3e-4\n        else:\n            lr = 5e-5\n        num_training_steps = self.num_epochs * len(train_dataloader)\n        gradient_accumulation_steps = 4\n        optimizer = torch.optim.AdamW(model.parameters(), lr = lr)\n        lr_scheduler = get_scheduler(\n            \"cosine\",\n            optimizer = optimizer,\n            num_warmup_steps = 100,\n            num_training_steps = num_training_steps,\n            )\n        progress_bar = tqdm(range(num_training_steps))\n\n        def eval(dataset):\n            model.eval()\n            total_loss = 0.0\n            for batch in dataset:\n                batch = {k:v.to(self.device) for k, v in batch.items()}\n                with torch.no_grad():\n                    outputs = model(**batch)\n                loss = outputs.loss\n                total_loss += loss.item()\n\n            return total_loss / len(dataset)\n\n\n        for epoch in range(self.num_epochs):\n            train_loss = 0.0\n            model.train()\n            for step, batch in enumerate(train_dataloader):\n                batch = {k:v.to(self.device) for k, v in batch.items()}\n                outputs = model(**batch)\n                loss = outputs.loss\n\n                train_loss += loss.item()\n\n                loss /= gradient_accumulation_steps\n                loss.backward()\n\n                if (step + 1) % gradient_accumulation_steps == 0:\n                    optimizer.step()\n                    optimizer.zero_grad()\n\n                lr_scheduler.step()\n                progress_bar.update(1)\n\n                if (step + 1) % self.logging_step == 0:\n                    avg_train_loss = train_loss / (step + 1)\n                    print(f'Epoch: {epoch + 1} -- step: {step + 1} -- avg_train_loss: {avg_train_loss} -- avg_train_ppl: {math.exp(avg_train_loss)}')\n                \n                    if ((step + 1) / self.logging_step) % 4 == 0:\n                        print(f'Saving at {self.model_name.split(\"/\")[1]}.checkpoint')\n                        torch.save(model.state_dict(), \"{}.checkpoint\".format(self.model_name.split(\"/\")[1]))\n\n                \n            print(\"Evaluating..............................\")\n            avg_train_loss = train_loss / len(train_dataloader)\n            avg_eval_loss = eval(valid_dataloader)\n            print(f'Epoch: {epoch + 1} -- avg_train_loss: {avg_train_loss} -- avg_val_loss: {avg_eval_loss} -- avg_train_ppl: {math.exp(avg_train_loss)} -- avg_val_ppl: {math.exp(avg_eval_loss)}')\n            print(\"================================================ End of epoch {} ================================================\".format(epoch + 1))\n\n\n            print(\"Saving..........\")\n            torch.save(model.state_dict(), \"{}.checkpoint\".format(self.model_name.split(\"/\")[1]))\n            print(\"****************** Save successfully ******************\")\n","metadata":{"id":"GBM-eAi8ge5L","outputId":"b6f19dad-f4c7-4b66-a1c1-dd79e3df1814","execution":{"iopub.status.busy":"2024-03-04T23:13:20.710604Z","iopub.execute_input":"2024-03-04T23:13:20.710896Z","iopub.status.idle":"2024-03-04T23:13:20.732514Z","shell.execute_reply.started":"2024-03-04T23:13:20.710870Z","shell.execute_reply":"2024-03-04T23:13:20.731568Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile run.py\nfrom train import Trainer\n\ntrainer = Trainer(\n    model_name=\"bkai-foundation-models/vietnamese-llama2-7b-40GB\",\n    dataset_name=\"NTCong/medical_qa_vn\",\n    num_epochs=10,\n    batch_size=1,\n    use_4bit=True,\n    use_peft=True,\n)\n\ntrainer.train()","metadata":{"id":"Gf7EBigQge-5","outputId":"065d249c-c38e-4554-b7af-b3b5f9edf1ee","execution":{"iopub.status.busy":"2024-03-04T23:15:12.692841Z","iopub.execute_input":"2024-03-04T23:15:12.693469Z","iopub.status.idle":"2024-03-04T23:15:12.698950Z","shell.execute_reply.started":"2024-03-04T23:15:12.693438Z","shell.execute_reply":"2024-03-04T23:15:12.698006Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!python run.py","metadata":{"id":"BJ5SWqat3lgX","outputId":"ad78d3e8-e376-49da-b156-5e3085a1edef","execution":{"iopub.status.busy":"2024-03-04T23:20:20.880538Z","iopub.execute_input":"2024-03-04T23:20:20.881390Z","iopub.status.idle":"2024-03-04T23:24:06.373831Z","shell.execute_reply.started":"2024-03-04T23:20:20.881355Z","shell.execute_reply":"2024-03-04T23:24:06.372660Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}